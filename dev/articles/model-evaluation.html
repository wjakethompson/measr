<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Evaluating diagnostic classification models • measr</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Open_Sans-0.4.10/font.css" rel="stylesheet">
<link href="../deps/Fira_Code-0.4.10/font.css" rel="stylesheet">
<link href="../deps/Playfair_Display-0.4.10/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Evaluating diagnostic classification models">
<meta name="robots" content="noindex">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-none" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">measr</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/measr.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/paper.html">measr: Bayesian psychometric measurement using Stan</a></li>
    <li><a class="dropdown-item" href="../articles/model-estimation.html">Estimating diagnostic classification models</a></li>
    <li><a class="dropdown-item" href="../articles/model-evaluation.html">Evaluating diagnostic classification models</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Case Studies</h6></li>
    <li><a class="dropdown-item" href="../articles/ecpe.html">Examination for the Certificate of Proficiency in English</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news">
<li><h6 class="dropdown-header" data-toc-skip>Releases</h6></li>
    <li><a class="external-link dropdown-item" href="https://www.wjakethompson.com/blog/measr/2024-02-measr-1.0.0/">Version 1.0.0</a></li>
    <li><a class="external-link dropdown-item" href="https://www.wjakethompson.com/blog/measr/2023-06-measr-0.3.1/">Version 0.3.1</a></li>
    <li><a class="external-link dropdown-item" href="https://www.wjakethompson.com/blog/measr/2023-04-measr-0.2.1/">Version 0.2.1</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../news/index.html">Changelog</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/wjakethompson/measr/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Evaluating diagnostic classification models</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/wjakethompson/measr/blob/main/vignettes/articles/model-evaluation.Rmd" class="external-link"><code>vignettes/articles/model-evaluation.Rmd</code></a></small>
      <div class="d-none name"><code>model-evaluation.Rmd</code></div>
    </div>

    
    
<p>In this article, we will describe the different options for
evaluating diagnostic classification models (DCMs; also known as
cognitive diagnostic models [CDMs]) using measr. We start with the data
to analyze, estimate our DCM, and learn how to evaluate different
aspects of the model such as model fit and reliability.</p>
<p>To use the code in this article, you will need to install and load
the measr package.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://measr.info">measr</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="example-data">Example Data<a class="anchor" aria-label="anchor" href="#example-data"></a>
</h2>
<p>To demonstrate the model fit functionality of measr, we’ll use the
same simulated data set that was used to illustrate <a href="model-estimation.html">model estimation functionality</a>. This
data set contains 2,000 respondents and 20 items that measure a total of
4 attributes, but no item measures more than 2 attributes. The data was
generated from the loglinear cognitive diagnostic model (LCDM), which is
a general model that subsumes many other DCM subtypes <span class="citation">(<a href="#ref-lcdm">Henson et al., 2009</a>)</span>.
To demonstrate model fit functionality, we’ll first fit an LCDM and a
deterministic-input, noisy “and” gate (DINA) model <span class="citation">(<a href="#ref-dina">de la Torre &amp; Douglas,
2004</a>)</span> to the data set in order to compare the fit indices.
Because the LCDM was used to generate our fake data, we expect our
estimated LCDM model to perform well. On the other hand, the DINA model
places heavy constraints on the LCDM, and therefore we expect worse
performance from the DINA model. For details on model estimation, see <a href="model-estimation.html">Estimating diagnostic classification
models</a>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org" class="external-link">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">sim_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://readr.tidyverse.org/reference/read_rds.html" class="external-link">read_rds</a></span><span class="op">(</span><span class="st">"data/simulated-data.rds"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lcdm</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dcm_estimate.html">dcm_estimate</a></span><span class="op">(</span></span>
<span>  dcm_spec <span class="op">=</span> <span class="fu"><a href="https://dcmstan.r-dcm.org/reference/dcm_specify.html" class="external-link">dcm_specify</a></span><span class="op">(</span>qmatrix <span class="op">=</span> <span class="va">sim_data</span><span class="op">$</span><span class="va">q_matrix</span>, identifier <span class="op">=</span> <span class="st">"item_id"</span>,</span>
<span>                         measurement_model <span class="op">=</span> <span class="fu"><a href="https://dcmstan.r-dcm.org/reference/measurement-model.html" class="external-link">lcdm</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  data <span class="op">=</span> <span class="va">sim_data</span><span class="op">$</span><span class="va">data</span>, identifier <span class="op">=</span> <span class="st">"resp_id"</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"mcmc"</span>, backend <span class="op">=</span> <span class="st">"cmdstanr"</span>,</span>
<span>  iter_warmup <span class="op">=</span> <span class="fl">1000</span>, iter_sampling <span class="op">=</span> <span class="fl">500</span>, chains <span class="op">=</span> <span class="fl">4</span>, parallel_chains <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  file <span class="op">=</span> <span class="st">"fits/sim-lcdm"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">dina</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dcm_estimate.html">dcm_estimate</a></span><span class="op">(</span></span>
<span>  dcm_spec <span class="op">=</span> <span class="fu"><a href="https://dcmstan.r-dcm.org/reference/dcm_specify.html" class="external-link">dcm_specify</a></span><span class="op">(</span>qmatrix <span class="op">=</span> <span class="va">sim_data</span><span class="op">$</span><span class="va">q_matrix</span>, identifier <span class="op">=</span> <span class="st">"item_id"</span>,</span>
<span>                         measurement_model <span class="op">=</span> <span class="fu"><a href="https://dcmstan.r-dcm.org/reference/measurement-model.html" class="external-link">dina</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  data <span class="op">=</span> <span class="va">sim_data</span><span class="op">$</span><span class="va">data</span>, identifier <span class="op">=</span> <span class="st">"resp_id"</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"mcmc"</span>, backend <span class="op">=</span> <span class="st">"cmdstanr"</span>,</span>
<span>  iter_warmup <span class="op">=</span> <span class="fl">1000</span>, iter_sampling <span class="op">=</span> <span class="fl">500</span>, chains <span class="op">=</span> <span class="fl">4</span>, parallel_chains <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  file <span class="op">=</span> <span class="st">"fits/sim-dina"</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-evaluation">Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"></a>
</h2>
<p>There are three major types of evaluations that are supported by
measr.</p>
<ul>
<li>Absolute model fit</li>
<li>Relative model fit (i.e., model comparisons)</li>
<li>Reliability</li>
</ul>
<p>Each of these are discussed in turn.</p>
<div class="section level3">
<h3 id="absolute-model-fit">Absolute Model Fit<a class="anchor" aria-label="anchor" href="#absolute-model-fit"></a>
</h3>
<p>Absolute model fit measures how well the estimated model fits the
observed data. One of the more popular methods for evaluating absolute
model fit for DCMs is the M<sub>2</sub> statistic <span class="citation">(<a href="#ref-hansen2016">Hansen et al., 2016</a>; <a href="#ref-liu2016">Liu et al., 2016</a>)</span>. We can calculate the
M<sub>2</sub> statistics with the <code><a href="https://rdrr.io/pkg/dcm2/man/fit_m2.html" class="external-link">fit_m2()</a></code> function.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/dcm2/man/fit_m2.html" class="external-link">fit_m2</a></span><span class="op">(</span><span class="va">lcdm</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 1 × 8</span></span></span>
<span><span class="co">#&gt;      m2    df  pval rmsea ci_lower ci_upper `90% CI`     srmsr</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>        <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span>  121.   129 0.672     0        0   0.009<span style="text-decoration: underline;">1</span> [0, 0.0091] 0.016<span style="text-decoration: underline;">6</span></span></span></code></pre></div>
<p>This function returns a data frame with the M<sub>2</sub> statistic
(<code>m2</code>), and the degrees of freedom and <em>p</em>-value
associated with the M<sub>2</sub> (<code>df</code> and
<code>pval</code>, respectively). A <em>p</em>-value less than .05 would
typically indicate poor model fit. As expected in this example, the
estimated LCDM shows adequate model fit to our data (<em>p</em> &gt;
.05). The <code><a href="https://rdrr.io/pkg/dcm2/man/fit_m2.html" class="external-link">fit_m2()</a></code> also return information on the RMSEA and
SRMSR fit statistics. The M<sub>2</sub>, RMSEA, and SRMSR are all
considered limited-information fit indices. For example, the
M<sub>2</sub> is based off of univariate item statistics and the
bivariate relationships between items. Thus, it cannot capture aspects
of model fit from higher-order relationships (e.g., triplets of
items).</p>
<p>Because we used a fully Bayesian estimation of our models, we can use
the posterior distributions to provide another evaluation of model fit
that can incorporate more information. This method is known as a
posterior predictive model check (PPMC). The general process for PPMCs
is as follows:</p>
<ol style="list-style-type: decimal">
<li>For each draw of the posterior distribution, generate a synthetic
data set using the parameters values for that draw.</li>
<li>For each synthetic data set calculate a summary of the data. This
process creates a posterior distribution of the summary. Because the
synthetic data sets are generated from the parameter values in the
posterior distribution, the distribution of the data summary represents
what we would expect the summary to look like, if the estimated model is
correct or true.</li>
<li>Calculate the same data summary for the observed data set.</li>
<li>Compare the summary from the observed data to the posterior
distribution.</li>
</ol>
<p>If the observed value falls within the posterior distribution of the
summary, this is evidence that our estimated model is consistent with
the observed data. On the other hand, discrepancies between the observed
value and posteriors indicates inconsistencies.</p>
<p>The PPMCs can be used to evaluate both model- and item-level fit. At
the model level, fit is evaluated by the expected raw score
distribution, as described by <span class="citation">Park et al. (<a href="#ref-park2015">2015</a>)</span> and <span class="citation">Thompson (<a href="#ref-thompson2019">2019</a>)</span>.
At the item level, we can evaluate fit by examining the expected
conditional probabilities of members of each class providing a correct
response <span class="citation">(<a href="#ref-sinharay2007">Sinharay
&amp; Almond, 2007</a>; <a href="#ref-thompson2019">Thompson,
2019</a>)</span>, as well as the expected odds ratio between each pair
of items <span class="citation">(<a href="#ref-park2015">Park et al.,
2015</a>; <a href="#ref-sinharay2006">Sinharay et al.,
2006</a>)</span>.</p>
<p>With measr, PPMCs can be calculated with <code><a href="../reference/fit_ppmc.html">fit_ppmc()</a></code>.
Using <code><a href="../reference/fit_ppmc.html">fit_ppmc()</a></code> can specify which PPMCs to calculate. For
example, here we estimate just the model-level raw score check by
setting <code>item_fit = NULL</code>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/fit_ppmc.html">fit_ppmc</a></span><span class="op">(</span><span class="va">lcdm</span>, model_fit <span class="op">=</span> <span class="st">"raw_score"</span><span class="op">)</span></span>
<span><span class="co">#&gt; $ppmc_raw_score</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 1 × 5</span></span></span>
<span><span class="co">#&gt;   obs_chisq ppmc_mean `2.5%` `97.5%`   ppp</span></span>
<span><span class="co">#&gt;       <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span>      24.0      23.7   10.9    40.9 0.452</span></span></code></pre></div>
<p><code><a href="../reference/fit_ppmc.html">fit_ppmc()</a></code> returns a list, where each element is a
different PPMC. Here, we only specified one PPMC, so we only get the
<code>raw_score</code> element back. The <code>obs_chisq</code> is the
raw score χ<sup>2</sup> values from our observed data. We then see the
mean of the posterior distribution for the χ<sup>2</sup>, quantiles of
the posterior distribution, and the posterior predictive
<em>p</em>-value (<em>ppp</em>). The <em>ppp</em> is the proportion of
posterior draws that are greater than the observed value. Values close
to 0 indicate poor fit. Values close to 1 may indicate overfitting.
Because the LCDM was used to generate this data, it’s not surprising
that the <em>ppp</em> is approaching close to 0.5 for our estimated
model, as the model is perfectly capturing the data generating process
(i.e., our observed data is right in the middle of what the estimated
model would expect).</p>
<p>We can also specify the posterior quantiles that are returned. For
example, we can calculate the item-level odds ratios and request
quantiles that will result in a 90% credible interval.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/fit_ppmc.html">fit_ppmc</a></span><span class="op">(</span><span class="va">lcdm</span>, model_fit <span class="op">=</span> <span class="cn">NULL</span>, item_fit <span class="op">=</span> <span class="st">"odds_ratio"</span>,</span>
<span>         probs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.05</span>, <span class="fl">0.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; $ppmc_odds_ratio</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 190 × 7</span></span></span>
<span><span class="co">#&gt;    item_1 item_2 obs_or ppmc_mean  `5%` `95%`   ppp</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> A1     A2      2.61       2.76 2.22   3.37 0.644</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> A1     A3      2.04       1.96 1.60   2.37 0.344</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> A1     A4      0.904      1.06 0.870  1.28 0.910</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> A1     A5      1.99       2.10 1.69   2.57 0.628</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> A1     A6      3.27       3.24 2.57   4.03 0.428</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> A1     A7      0.966      1.05 0.864  1.25 0.747</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> A1     A8      6.78       6.56 5.06   8.38 0.376</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> A1     A9     10.1        9.76 7.56  12.4  0.359</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> A1     A10     2.98       2.85 2.21   3.68 0.352</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> A1     A11     2.98       3.06 2.50   3.71 0.550</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 180 more rows</span></span></span></code></pre></div>
<p>We see similar output for the item-level indices: the observed odds
ratio for each item pair (<code>obs_or</code>), the mean of the
posterior distribution for each item pair (<code>ppmc_mean</code>), the
quantiles of the posterior that we specified, and the <em>ppp</em>. As
with the raw score distribution, here the <em>ppp</em> represents the
proportion of posterior draws where the odds ratio is greater than the
observed value.</p>
</div>
<div class="section level3">
<h3 id="relative-model-fit">Relative Model Fit<a class="anchor" aria-label="anchor" href="#relative-model-fit"></a>
</h3>
<p>Relative fit measures are used to compare multiple competing models.
In this case, we have estimate an LCDM and a DINA model and want to
compare which model performs better. We should first note that “better”
does not necessarily mean “good.” Evidence that one model performs
better than another is not evidence of “good” fit in an absolute sense.
Only absolute model fit indices can provide evidence of adequate fit to
the data. However, if multiple models show adequate absolute fit,
relative fit indices can be used, along with our understanding of the
constructs, to select a preferred model.</p>
<p>Currently, the <em>Stan</em> ecosystem supports two information
criteria through the <a href="https://mc-stan.org/loo/" class="external-link">loo</a> package
that can be used as relative fit indices: leave-one-out (LOO) cross
validation with Pareto-smoothed importance sampling <span class="citation">(<a href="#ref-loo-waic">Vehtari et al., 2017</a>, <a href="#ref-psis">2022</a>)</span> and the widely applicable information
criterion (WAIC) described by <span class="citation">Watanabe (<a href="#ref-waic">2010</a>)</span>. The information criteria can be
calculated using the associated functions from the loo package (i.e.,
<code><a href="https://mc-stan.org/loo/reference/loo.html" class="external-link">loo()</a></code> and <code><a href="https://mc-stan.org/loo/reference/waic.html" class="external-link">waic()</a></code>). Here, we calculate the LOO
for both the LCDM and DINA models.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lcdm_loo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/loo/reference/loo.html" class="external-link">loo</a></span><span class="op">(</span><span class="va">lcdm</span><span class="op">)</span></span>
<span><span class="va">lcdm_loo</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Computed from 2000 by 2000 log-likelihood matrix.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;          Estimate    SE</span></span>
<span><span class="co">#&gt; elpd_loo -21731.5 102.7</span></span>
<span><span class="co">#&gt; p_loo        77.8   1.2</span></span>
<span><span class="co">#&gt; looic     43463.0 205.4</span></span>
<span><span class="co">#&gt; ------</span></span>
<span><span class="co">#&gt; MCSE of elpd_loo is 0.2.</span></span>
<span><span class="co">#&gt; MCSE and ESS estimates assume independent draws (r_eff=1).</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; All Pareto k estimates are good (k &lt; 0.7).</span></span>
<span><span class="co">#&gt; See help('pareto-k-diagnostic') for details.</span></span>
<span></span>
<span><span class="va">dina_loo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/loo/reference/loo.html" class="external-link">loo</a></span><span class="op">(</span><span class="va">dina</span><span class="op">)</span></span>
<span><span class="va">dina_loo</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Computed from 2000 by 2000 log-likelihood matrix.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;          Estimate    SE</span></span>
<span><span class="co">#&gt; elpd_loo -23539.8  79.1</span></span>
<span><span class="co">#&gt; p_loo       886.9  18.9</span></span>
<span><span class="co">#&gt; looic     47079.5 158.3</span></span>
<span><span class="co">#&gt; ------</span></span>
<span><span class="co">#&gt; MCSE of elpd_loo is 0.6.</span></span>
<span><span class="co">#&gt; MCSE and ESS estimates assume independent draws (r_eff=1).</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; All Pareto k estimates are good (k &lt; 0.7).</span></span>
<span><span class="co">#&gt; See help('pareto-k-diagnostic') for details.</span></span></code></pre></div>
<p>In isolation, this output is not very useful, as these indices are
meant to facilitate model comparisons. We can conduct model comparisons
using <code><a href="https://mc-stan.org/loo/reference/loo_compare.html" class="external-link">loo_compare()</a></code>, which is used for comparing both LOO
and WAIC estimates. In the output, the model in the first row is the
preferred model. In subsequent rows, the <code>epld_diff</code> column
reports the difference in the information criteria (in this case the
LOO) between the model in that row and the preferred model. The
<code>se_diff</code> column is the standard error of the difference
between that model and the preferred model.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://mc-stan.org/loo/reference/loo_compare.html" class="external-link">loo_compare</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lcdm <span class="op">=</span> <span class="va">lcdm_loo</span>, dina <span class="op">=</span> <span class="va">dina_loo</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;      elpd_diff se_diff</span></span>
<span><span class="co">#&gt; lcdm     0.0       0.0</span></span>
<span><span class="co">#&gt; dina -1808.3      45.1</span></span></code></pre></div>
<p><span class="citation">Bengio &amp; Grandvalet (<a href="#ref-bengio2004">2004</a>)</span> have recommended a cutoff of 2.5
standard errors for identifying the preferred model. For example,
because the absolute value of the <code>elpd_diff</code> is greater than
45.1 × 2.5 = 112.8, we would conclude that the LCDM fits significantly
better than the DINA model. This is our expected outcome in this case,
given the data generation process and the results of the absolute model
fit analysis. If the difference were less than 2.5 standard errors, we
would conclude that the models fit equally well.</p>
</div>
<div class="section level3">
<h3 id="reliability">Reliability<a class="anchor" aria-label="anchor" href="#reliability"></a>
</h3>
<p>We can also evaluate DCMs through their reliability. That is, it’s
important to understand the accuracy and consistency of the
classifications that are made by the model. For models estimated with
measr, estimates of reliability can be calculated using
<code><a href="../reference/reliability.html">reliability()</a></code>.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/reliability.html">reliability</a></span><span class="op">(</span><span class="va">lcdm</span><span class="op">)</span></span>
<span><span class="co">#&gt; $pattern_reliability</span></span>
<span><span class="co">#&gt;       p_a       p_c </span></span>
<span><span class="co">#&gt; 0.7444377 0.6093438 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $map_reliability</span></span>
<span><span class="co">#&gt; $map_reliability$accuracy</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 4 × 8</span></span></span>
<span><span class="co">#&gt;   attribute   acc lambda_a kappa_a youden_a tetra_a  tp_a  tn_a</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> att1      0.931    0.857   0.244    0.862   0.977 0.929 0.933</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> att2      0.980    0.955   0.944    0.960   0.998 0.981 0.979</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> att3      0.835    0.633   0.650    0.659   0.869 0.882 0.777</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span> att4      0.966    0.931   0.395    0.932   0.994 0.968 0.964</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $map_reliability$consistency</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 4 × 10</span></span></span>
<span><span class="co">#&gt;   attribute consist lambda_c kappa_c youden_c tetra_c  tp_c  tn_c gammak</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> att1        0.873    0.737   0.860    0.745   0.921 0.868 0.877  0.898</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> att2        0.960    0.909   0.935    0.919   0.992 0.955 0.964  0.970</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> att3        0.761    0.423   0.623    0.507   0.718 0.796 0.711  0.771</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span> att4        0.936    0.870   0.932    0.871   0.980 0.935 0.936  0.948</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 1 more variable: pc_prime &lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $eap_reliability</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 4 × 5</span></span></span>
<span><span class="co">#&gt;   attribute rho_pf rho_bs rho_i rho_tb</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> att1       0.799  0.797 0.647  0.949</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> att2       0.937  0.938 0.717  0.995</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> att3       0.619  0.537 0.480  0.748</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span> att4       0.902  0.897 0.700  0.987</span></span></code></pre></div>
<p>There are several types of reliability evidence that are provided in
the output. For all indices reported, values can range from 0 to 1,
where 1 represents perfect accuracy or consistency. The first type
reliability in the output is pattern-level reliability, as described by
<span class="citation">Cui et al. (<a href="#ref-cui2012">2012</a>)</span>. This reliability describes the
accuracy (<code>p_a</code>) and consistency (<code>p_c</code>) of the
classification of respondents into an overall profile of proficiency on
the on assessed skills. For example, in a 3-attribute assessment there
are 8 possible profiles: [0,0,0], [1,0,0], [0,1,0], [0,0,1], [1,1,0],
[1,0,1], [0,1,1], [1,1,1]. One option for reporting results from a
DCM-based assessment is to select the profile that is most likely for
each respondent. These pattern-level reliability metrics provide a
measure of the accuracy and consistency for this type of
classification.</p>
<p>On the other hand, rather than reporting results based on the overall
most likely profile, we can assign proficiency for each individual
attribute, and build up a overall profile from the attribute-level
decisions. This type of reporting may or may not result in the same
profile as the overall most likely profile. Because in this scenario
classifications are made at the attribute level, we need to examine the
classification accuracy and consistency for each individual attribute.
For models estimated with measr, this is referred to <em>maximum a
posteriori</em> (MAP) reliability, because classifications are based on
the most likely category for each attribute for each respondent. The MAP
values reported in the output are those described by <span class="citation">Johnson &amp; Sinharay (<a href="#ref-johnson2018">2018</a>)</span>. The <code>acc</code> and
<code>consist</code> variables in <code>map_reliability$accuracy</code>
and <code>map_reliability_consistency</code> tables, respectively, are
the classification accuracy and consistency metrics described by <span class="citation">Johnson &amp; Sinharay (<a href="#ref-johnson2018">2018</a>)</span>. In their paper, they also
compared these metrics to other measures of agreement (e.g., Cohen’s κ,
Goodman and Kruskal’s λ), which are also included in the output. <span class="citation">Johnson &amp; Sinharay (<a href="#ref-johnson2018">2018</a>)</span> also provide recommendations
for threshold values for each metric that represent poor, fair, good,
very good, or excellent reliability.</p>
<p>Finally, rather than reporting results as classifications (e.g.,
proficient/not proficient), results can also be reported simply as the
probability that each respondent is proficient on each attribute. Thus,
rather than reporting the accuracy and consistency of a classification,
we should report the precision of the reported probability. This is
referred to as <em>expected a posteriori</em> (EAP) reliability, because
the probability represents the expected value of each attribute for each
respondent. The EAP values reported in the output
(<code>eap_reliability</code>) are those described by <span class="citation">Johnson &amp; Sinharay (<a href="#ref-johnson2020">2020</a>)</span> and <span class="citation">Templin &amp; Bradshaw (<a href="#ref-templin2013">2013</a>)</span>. <span class="citation">Templin
&amp; Bradshaw (<a href="#ref-templin2013">2013</a>)</span> describe a
reliability index based on a restrictive assumption of parallel forms
(<code>rho_tb</code>). <span class="citation">Johnson &amp; Sinharay (<a href="#ref-johnson2020">2020</a>)</span> described a more generalized
parallel forms reliability (<code>rho_pf</code>), along with additional
biserial (<code>rho_bs</code>), and informational (<code>rho_i</code>)
reliability indices. Because proficiency probabilities are provided at
the attribute level, the EAP reliability estimates are also provided for
each attribute measured by the assessment. As with the classification
reliability indices, <span class="citation">Johnson &amp; Sinharay (<a href="#ref-johnson2020">2020</a>)</span> provide recommendations for
thresholds representing poor, fair, good, very good, and excellent
reliability for all four EAP reliability indices.</p>
</div>
</div>
<div class="section level2">
<h2 id="storing-model-evaluations">Storing Model Evaluations<a class="anchor" aria-label="anchor" href="#storing-model-evaluations"></a>
</h2>
<p>If you have followed along with running the code in this vignette,
you will have noticed that some of the model evaluations take a
significant amount of computation time. This means repeating the
calculations (e.g., didn’t assign the output new a new object, opened a
new R session) can be a very time-consuming process. To make you
analysis more efficient, measr offers several functions that can be used
to add the model evaluation metrics described in this vignette directly
to the model object. If you specified a <code>file</code> when the model
was estimated, the updated model object with the model evaluation
components will automatically resave, ensuring that you don’t have to
rerun computationally intensive tasks.</p>
<p>There are three functions for adding model evaluation components to a
model object, which correspond to the three types of evaluation
described in this vignette:</p>
<ul>
<li>
<code><a href="../reference/model_evaluation.html">add_fit()</a></code>: Adds absolute model fit indices (i.e.,
M<sub>2</sub>, PPMCs)</li>
<li>
<code><a href="../reference/model_evaluation.html">add_criterion()</a></code>: Adds relative model fit indices (i.e.,
LOO, WAIC)</li>
<li>
<code><a href="../reference/model_evaluation.html">add_reliability()</a></code>: Adds reliability metrics</li>
</ul>
<p>All three functions have several arguments in common.</p>
<ul>
<li>
<code>x</code>: The model to add evaluation components to.</li>
<li>
<code>save</code>: Whether to resave the model object if
<code>file</code> was specified when estimating the model. The default
is <code>TRUE</code>.</li>
<li>
<code>overwrite</code>: Whether to overwrite existing evaluations.
For example, if you attempt to add reliability metrics with
<code><a href="../reference/model_evaluation.html">add_reliability()</a></code>, but those metrics have already been
added, should the reliability metrics be recalculated and overwrite the
existing metrics? The default is <code>FALSE</code>.</li>
</ul>
<p>Additionally, all three functions have a <code>...</code> argument
for passing additional arguments along to the relevant functions. For
example, if we want to add PPMC absolute model fit indices, we can
specify the types of model- and item-level fit indices to calculate,
just as we did when using <code><a href="../reference/fit_ppmc.html">fit_ppmc()</a></code>.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lcdm</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_evaluation.html">add_fit</a></span><span class="op">(</span><span class="va">lcdm</span>, method <span class="op">=</span> <span class="st">"ppmc"</span>,</span>
<span>                model_fit <span class="op">=</span> <span class="st">"raw_score"</span>,</span>
<span>                item_fit <span class="op">=</span> <span class="st">"odds_ratio"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Warning in file.remove(current_files[!current_files %in% new_paths]):</span></span>
<span><span class="co">#&gt; cannot remove file</span></span>
<span><span class="co">#&gt; '/Users/jakethompson/Documents/GIT/packages/measr/vignettes/articles/fits/sim-lcdm-1.csv',</span></span>
<span><span class="co">#&gt; reason 'No such file or directory'</span></span>
<span><span class="co">#&gt; Warning in file.remove(current_files[!current_files %in% new_paths]):</span></span>
<span><span class="co">#&gt; cannot remove file</span></span>
<span><span class="co">#&gt; '/Users/jakethompson/Documents/GIT/packages/measr/vignettes/articles/fits/sim-lcdm-2.csv',</span></span>
<span><span class="co">#&gt; reason 'No such file or directory'</span></span>
<span><span class="co">#&gt; Warning in file.remove(current_files[!current_files %in% new_paths]):</span></span>
<span><span class="co">#&gt; cannot remove file</span></span>
<span><span class="co">#&gt; '/Users/jakethompson/Documents/GIT/packages/measr/vignettes/articles/fits/sim-lcdm-3.csv',</span></span>
<span><span class="co">#&gt; reason 'No such file or directory'</span></span>
<span><span class="co">#&gt; Warning in file.remove(current_files[!current_files %in% new_paths]):</span></span>
<span><span class="co">#&gt; cannot remove file</span></span>
<span><span class="co">#&gt; '/Users/jakethompson/Documents/GIT/packages/measr/vignettes/articles/fits/sim-lcdm-4.csv',</span></span>
<span><span class="co">#&gt; reason 'No such file or directory'</span></span>
<span><span class="co">#&gt; Moved 4 files and set internal paths to new locations:</span></span>
<span><span class="co">#&gt; - /home/runner/work/measr/measr/vignettes/articles/NA</span></span>
<span><span class="co">#&gt; - /home/runner/work/measr/measr/vignettes/articles/NA</span></span>
<span><span class="co">#&gt; - /home/runner/work/measr/measr/vignettes/articles/NA</span></span>
<span><span class="co">#&gt; - /home/runner/work/measr/measr/vignettes/articles/NA</span></span></code></pre></div>
<p>Once components have been added to the model, a helper function,
<code><a href="../reference/measr_extract.html">measr_extract()</a></code>, can be used to pull out relevant pieces of
output. For example, we can extract the PPMC raw score results.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/measr_extract.html">measr_extract</a></span><span class="op">(</span><span class="va">lcdm</span>, what <span class="op">=</span> <span class="st">"ppmc_raw_score"</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 1 × 5</span></span></span>
<span><span class="co">#&gt;   obs_chisq ppmc_mean `2.5%` `97.5%`   ppp</span></span>
<span><span class="co">#&gt;       <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span>      24.0      23.7   10.9    40.9 0.452</span></span></code></pre></div>
<p>In addition, we can also extract elements of the model, such as the
priors that were used during estimation, or estimated parameters like
the base rate of membership in each class.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/measr_extract.html">measr_extract</a></span><span class="op">(</span><span class="va">lcdm</span>, what <span class="op">=</span> <span class="st">"prior"</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 4 × 3</span></span></span>
<span><span class="co">#&gt;   type        coefficient prior                      </span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> intercept   <span style="color: #BB0000;">NA</span>          normal(0, 2)               </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> maineffect  <span style="color: #BB0000;">NA</span>          lognormal(0, 1)            </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> interaction <span style="color: #BB0000;">NA</span>          normal(0, 2)               </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span> structural  Vc          dirichlet(rep_vector(1, C))</span></span>
<span></span>
<span><span class="fu"><a href="../reference/measr_extract.html">measr_extract</a></span><span class="op">(</span><span class="va">lcdm</span>, what <span class="op">=</span> <span class="st">"strc_param"</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 16 × 2</span></span></span>
<span><span class="co">#&gt;    class            estimate</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>          <span style="color: #949494; font-style: italic;">&lt;rvar[1d]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> [0,0,0,0]  0.059 <span style="color: #949494;">± 0.0069</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> [1,0,0,0]  0.080 <span style="color: #949494;">± 0.0075</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> [0,1,0,0]  0.063 <span style="color: #949494;">± 0.0065</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> [0,0,1,0]  0.083 <span style="color: #949494;">± 0.0078</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> [0,0,0,1]  0.052 <span style="color: #949494;">± 0.0086</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> [1,1,0,0]  0.044 <span style="color: #949494;">± 0.0059</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> [1,0,1,0]  0.051 <span style="color: #949494;">± 0.0066</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> [1,0,0,1]  0.060 <span style="color: #949494;">± 0.0110</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> [0,1,1,0]  0.083 <span style="color: #949494;">± 0.0078</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> [0,1,0,1]  0.049 <span style="color: #949494;">± 0.0079</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">11</span> [0,0,1,1]  0.081 <span style="color: #949494;">± 0.0100</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">12</span> [1,1,1,0]  0.043 <span style="color: #949494;">± 0.0066</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">13</span> [1,1,0,1]  0.044 <span style="color: #949494;">± 0.0095</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">14</span> [1,0,1,1]  0.092 <span style="color: #949494;">± 0.0111</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">15</span> [0,1,1,1]  0.047 <span style="color: #949494;">± 0.0085</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">16</span> [1,1,1,1]  0.068 <span style="color: #949494;">± 0.0099</span></span></span></code></pre></div>
<p>For a complete list of what can be extract with
<code><a href="../reference/measr_extract.html">measr_extract()</a></code>, see <code><a href="../reference/measr_extract.html">?measr_extract</a></code>.</p>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-bengio2004" class="csl-entry">
Bengio, Y., &amp; Grandvalet, Y. (2004). No unbiased estimator of the
variance of k-fold cross-validation. <em>Journal of Machine
Learning</em>, <em>5</em>, 1089–1105. <a href="http://www.jmlr.org/papers/v5/grandvalet04a.html" class="external-link">http://www.jmlr.org/papers/v5/grandvalet04a.html</a>
</div>
<div id="ref-cui2012" class="csl-entry">
Cui, Y., Gierl, M. J., &amp; Chang, H.-H. (2012). Estimating
classification consistency and accuracy for cognitive diagnostic
assessment. <em>Journal of Educational Measurement</em>, <em>49</em>(1),
19–38. <a href="https://doi.org/10.1111/j.1745-3984.2011.00158.x" class="external-link">https://doi.org/10.1111/j.1745-3984.2011.00158.x</a>
</div>
<div id="ref-dina" class="csl-entry">
de la Torre, J., &amp; Douglas, J. A. (2004). Higher-order latent trait
models for cognitive diagnosis. <em>Psychometrika</em>, <em>69</em>(3),
333–353. <a href="https://doi.org/10.1007/BF02295640" class="external-link">https://doi.org/10.1007/BF02295640</a>
</div>
<div id="ref-hansen2016" class="csl-entry">
Hansen, M., Cai, L., Monroe, S., &amp; Li, Z. (2016).
Limited-information goodness-of-fit testing of diagnostic classification
item response models. <em>British Journal of Mahtematical and
Statistical Psychology</em>, <em>69</em>(3), 225–252. <a href="https://doi.org/10.1111/bmsp.12074" class="external-link">https://doi.org/10.1111/bmsp.12074</a>
</div>
<div id="ref-lcdm" class="csl-entry">
Henson, R., Templin, J., &amp; Willse, J. T. (2009). Defining a family
of cognitive diagnosis models using log-linear models with latent
variables. <em>Psychometrika</em>, <em>74</em>(2), 191–210. <a href="https://doi.org/10.1007/s11336-008-9089-5" class="external-link">https://doi.org/10.1007/s11336-008-9089-5</a>
</div>
<div id="ref-johnson2018" class="csl-entry">
Johnson, M. S., &amp; Sinharay, S. (2018). Measures of agreement to
assess attribute-level classification accuracy and consistency for
cognitive diagnostic assessments. <em>Journal of Educational
Measurement</em>, <em>55</em>(4), 635–664. <a href="https://doi.org/10.1111/jedm.12196" class="external-link">https://doi.org/10.1111/jedm.12196</a>
</div>
<div id="ref-johnson2020" class="csl-entry">
Johnson, M. S., &amp; Sinharay, S. (2020). The reliability of the
posterior probability of skill attainment in diagnostic classification
models. <em>Journal of Educational and Behavioral Statistics</em>,
<em>45</em>(1), 5–31. <a href="https://doi.org/10.3102/1076998619864550" class="external-link">https://doi.org/10.3102/1076998619864550</a>
</div>
<div id="ref-liu2016" class="csl-entry">
Liu, Y., Tian, W., &amp; Xin, T. (2016). An application of
<span><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>M</mi><mn>2</mn></msub><annotation encoding="application/x-tex">M_2</annotation></semantics></math></span>
statistic to evaluate the fit of cognitive diagnostic models.
<em>Journal of Educational and Behavioral Statistics</em>,
<em>41</em>(1), 3–26. <a href="https://doi.org/10.3102/1076998615621293" class="external-link">https://doi.org/10.3102/1076998615621293</a>
</div>
<div id="ref-park2015" class="csl-entry">
Park, J. Y., Johnson, M. S., &amp; Lee, Y.-S. (2015). Posterior
predictive model checks for cognitive diagnostic models.
<em>International Journal of Quantitative Research in Education</em>,
<em>2</em>(3–4), 244–264. <a href="https://doi.org/10.1504/IJQRE.2015.071738" class="external-link">https://doi.org/10.1504/IJQRE.2015.071738</a>
</div>
<div id="ref-sinharay2007" class="csl-entry">
Sinharay, S., &amp; Almond, R. G. (2007). Assessing fit of cognitive
diagnostic models. <em>Educational and Psychological Measurement</em>,
<em>67</em>(2), 239–257. <a href="https://doi.org/10.1177/0013164406292025" class="external-link">https://doi.org/10.1177/0013164406292025</a>
</div>
<div id="ref-sinharay2006" class="csl-entry">
Sinharay, S., Johnson, M. S., &amp; Stern, H. S. (2006). Posterior
predictive assessment of item response theory models. <em>Applied
Psychological Measurement</em>, <em>30</em>(4), 298–321. <a href="https://doi.org/10.1177/0146621605285517" class="external-link">https://doi.org/10.1177/0146621605285517</a>
</div>
<div id="ref-templin2013" class="csl-entry">
Templin, J., &amp; Bradshaw, L. (2013). Measuring the reliability of
diagnostic classification model examinee estimates. <em>Journal of
Classification</em>, <em>30</em>(2), 251–275. <a href="https://doi.org/10.1007/s00357-013-9129-4" class="external-link">https://doi.org/10.1007/s00357-013-9129-4</a>
</div>
<div id="ref-thompson2019" class="csl-entry">
Thompson, W. J. (2019). <em>Bayesian psychometrics for diagnostic
assessments: <span>A</span> proof of concept</em> (Research Report No.
19-01). <span>University of Kansas; Accessible Teaching, Learning, and
Assessment Systems</span>. <a href="https://doi.org/10.35542/osf.io/jzqs8" class="external-link">https://doi.org/10.35542/osf.io/jzqs8</a>
</div>
<div id="ref-loo-waic" class="csl-entry">
Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical
<span>Bayesian</span> model evaluation using leave-one-out
cross-validation and <span>WAIC</span>. <em>Statistics and
Computing</em>, <em>27</em>, 1413–1432. <a href="https://doi.org/10.1007/s11222-016-9696-4" class="external-link">https://doi.org/10.1007/s11222-016-9696-4</a>
</div>
<div id="ref-psis" class="csl-entry">
Vehtari, A., Simpson, D., Gelman, A., Yao, Y., &amp; Gabry, J. (2022).
<em>Pareto smoothed importance sampling</em>. <a href="https://doi.org/10.48550/arXiv.1507.02646" class="external-link">https://doi.org/10.48550/arXiv.1507.02646</a>
</div>
<div id="ref-waic" class="csl-entry">
Watanabe, S. (2010). Asymptotic equivalence of <span>Bayes</span> cross
validation and widely applicable information criterion in singular
learning theory. <em>Journal of Machine Learning Research</em>,
<em>11</em>, 3571–3594. <a href="http://www.jmlr.org/papers/v11/watanabe10a.html" class="external-link">http://www.jmlr.org/papers/v11/watanabe10a.html</a>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://wjakethompson.com" class="external-link">W. Jake Thompson</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
